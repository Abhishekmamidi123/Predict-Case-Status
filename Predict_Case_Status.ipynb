{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Predict Case Status </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "le = LabelEncoder()\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set display options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = 'dataset/'\n",
    "TRAIN_PATH = FOLDER + 'train.csv'\n",
    "TEST_PATH = FOLDER + 'test.csv'\n",
    "SAMPLE_SUBMISSION_PATH = FOLDER + 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_NO</th>\n",
       "      <th>CASE_SUBMITTED_DAY</th>\n",
       "      <th>CASE_SUBMITTED_MONTH</th>\n",
       "      <th>CASE_SUBMITTED_YEAR</th>\n",
       "      <th>DECISION_DAY</th>\n",
       "      <th>DECISION_MONTH</th>\n",
       "      <th>DECISION_YEAR</th>\n",
       "      <th>VISA_CLASS</th>\n",
       "      <th>EMPLOYER_NAME</th>\n",
       "      <th>EMPLOYER_STATE</th>\n",
       "      <th>EMPLOYER_COUNTRY</th>\n",
       "      <th>SOC_NAME</th>\n",
       "      <th>NAICS_CODE</th>\n",
       "      <th>TOTAL_WORKERS</th>\n",
       "      <th>FULL_TIME_POSITION</th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>PW_UNIT_OF_PAY</th>\n",
       "      <th>PW_SOURCE</th>\n",
       "      <th>PW_SOURCE_YEAR</th>\n",
       "      <th>PW_SOURCE_OTHER</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "      <th>WAGE_UNIT_OF_PAY</th>\n",
       "      <th>H-1B_DEPENDENT</th>\n",
       "      <th>WILLFUL_VIOLATOR</th>\n",
       "      <th>WORKSITE_STATE</th>\n",
       "      <th>WORKSITE_POSTAL_CODE</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9831643</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2011</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>H1B</td>\n",
       "      <td>XTRON SOFTWARE SERVICES INC</td>\n",
       "      <td>CA</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>ANALYSTS</td>\n",
       "      <td>541511.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Y</td>\n",
       "      <td>70907.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>OES</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>OFLC ONLINE DATA CENTER</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>91000.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>CA</td>\n",
       "      <td>95054</td>\n",
       "      <td>CERTIFIEDWITHDRAWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5075895</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2011</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>H1B</td>\n",
       "      <td>GLOBAL DISTRIBUTORS INC</td>\n",
       "      <td>MD</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>ACCOUNTANTS</td>\n",
       "      <td>423210.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>50877.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>OES</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>OFLC ONLINE DATA CENTER</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>MD</td>\n",
       "      <td>20814</td>\n",
       "      <td>CERTIFIEDWITHDRAWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4040912</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>H1B</td>\n",
       "      <td>MARSHALL WEALTH MANAGEMENT LLC</td>\n",
       "      <td>KY</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>ACTUARIES</td>\n",
       "      <td>523930.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Y</td>\n",
       "      <td>28454.4</td>\n",
       "      <td>Year</td>\n",
       "      <td>OES</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>OFLC ONLINE DATA CENTER</td>\n",
       "      <td>28455.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>KY</td>\n",
       "      <td>40202</td>\n",
       "      <td>WITHDRAWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4491642</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>H1B</td>\n",
       "      <td>XTRON SOFTWARE SERVICES INC</td>\n",
       "      <td>CA</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>ANALYSTS</td>\n",
       "      <td>541511.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Y</td>\n",
       "      <td>61963.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>OES</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>OFLC ONLINE DATA CENTER</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>CA</td>\n",
       "      <td>95054</td>\n",
       "      <td>CERTIFIEDWITHDRAWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3332446</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>H1B</td>\n",
       "      <td>XTRON SOFTWARE SERVICES INC</td>\n",
       "      <td>CA</td>\n",
       "      <td>UNITED STATES OF AMERICA</td>\n",
       "      <td>ANALYSTS</td>\n",
       "      <td>541511.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Y</td>\n",
       "      <td>71947.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>OES</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>OFLC ONLINE DATA CENTER</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>92000.0</td>\n",
       "      <td>Year</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>CA</td>\n",
       "      <td>95054</td>\n",
       "      <td>CERTIFIEDWITHDRAWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CASE_NO  CASE_SUBMITTED_DAY  CASE_SUBMITTED_MONTH  CASE_SUBMITTED_YEAR  \\\n",
       "0  9831643                23.0                     3                 2011   \n",
       "1  5075895                28.0                     3                 2011   \n",
       "2  4040912                17.0                     2                 2012   \n",
       "3  4491642                22.0                     3                 2012   \n",
       "4  3332446                22.0                     3                 2012   \n",
       "\n",
       "   DECISION_DAY  DECISION_MONTH  DECISION_YEAR VISA_CLASS  \\\n",
       "0            14               4           2017        H1B   \n",
       "1            10               3           2017        H1B   \n",
       "2            18              10           2016        H1B   \n",
       "3            14               4           2017        H1B   \n",
       "4            14               4           2017        H1B   \n",
       "\n",
       "                    EMPLOYER_NAME EMPLOYER_STATE          EMPLOYER_COUNTRY  \\\n",
       "0     XTRON SOFTWARE SERVICES INC             CA  UNITED STATES OF AMERICA   \n",
       "1         GLOBAL DISTRIBUTORS INC             MD  UNITED STATES OF AMERICA   \n",
       "2  MARSHALL WEALTH MANAGEMENT LLC             KY  UNITED STATES OF AMERICA   \n",
       "3     XTRON SOFTWARE SERVICES INC             CA  UNITED STATES OF AMERICA   \n",
       "4     XTRON SOFTWARE SERVICES INC             CA  UNITED STATES OF AMERICA   \n",
       "\n",
       "      SOC_NAME  NAICS_CODE  TOTAL_WORKERS FULL_TIME_POSITION  PREVAILING_WAGE  \\\n",
       "0     ANALYSTS    541511.0              5                  Y          70907.0   \n",
       "1  ACCOUNTANTS    423210.0              1                  Y          50877.0   \n",
       "2    ACTUARIES    523930.0              1                  Y          28454.4   \n",
       "3     ANALYSTS    541511.0              5                  Y          61963.0   \n",
       "4     ANALYSTS    541511.0              5                  Y          71947.0   \n",
       "\n",
       "  PW_UNIT_OF_PAY PW_SOURCE  PW_SOURCE_YEAR          PW_SOURCE_OTHER  \\\n",
       "0           Year       OES          2011.0  OFLC ONLINE DATA CENTER   \n",
       "1           Year       OES          2010.0  OFLC ONLINE DATA CENTER   \n",
       "2           Year       OES          2011.0  OFLC ONLINE DATA CENTER   \n",
       "3           Year       OES          2012.0  OFLC ONLINE DATA CENTER   \n",
       "4           Year       OES          2012.0  OFLC ONLINE DATA CENTER   \n",
       "\n",
       "   WAGE_RATE_OF_PAY_FROM  WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY H-1B_DEPENDENT  \\\n",
       "0                71000.0              91000.0             Year              Y   \n",
       "1                51000.0                  0.0             Year              N   \n",
       "2                28455.0                  0.0             Year              N   \n",
       "3                62000.0              82000.0             Year              Y   \n",
       "4                72000.0              92000.0             Year              Y   \n",
       "\n",
       "  WILLFUL_VIOLATOR WORKSITE_STATE WORKSITE_POSTAL_CODE         CASE_STATUS  \n",
       "0                N             CA                95054  CERTIFIEDWITHDRAWN  \n",
       "1                N             MD                20814  CERTIFIEDWITHDRAWN  \n",
       "2                N             KY                40202           WITHDRAWN  \n",
       "3                N             CA                95054  CERTIFIEDWITHDRAWN  \n",
       "4                N             CA                95054  CERTIFIEDWITHDRAWN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Train and Test data\n",
    "- Drop unnecessary features/columns.\n",
    "- Handle null values.\n",
    "- Create new features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CASE_SUBMITTED_DAY has 3999 null values. Since, this column has day information, I imputed missing values \n",
    "by ramdomly sampling numbers between 1 to 28 (if it is between 1 to 31, some of the months may not have that \n",
    "many days).\n",
    "'''\n",
    "null_case_submitted_day = train_data[train_data['CASE_SUBMITTED_DAY'].isnull()]\n",
    "null_case_submitted_day.CASE_SUBMITTED_DAY = null_case_submitted_day['CASE_SUBMITTED_DAY'].apply(lambda v: random.randint(1,28))\n",
    "train_data[train_data['CASE_SUBMITTED_DAY'].isnull()] = null_case_submitted_day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create date-time features \"CASE_SUBMITTED_DATE\", \"DECISION_DATE\" and \"DECISION_PERIOD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_period_column(train_data):\n",
    "    '''\n",
    "    Combined (CASE_SUBMITTED_DAY, CASE_SUBMITTED_MONTH, CASE_SUBMITTED_YEAR) and (DECISION_DAY, DECISION_MONTH, DECISION_YEAR) \n",
    "    to get CASE_SUBMITTED_DATE and DECISION_DATE date-time features respectively.\n",
    "    '''\n",
    "    train_data['CASE_SUBMITTED_DAY'] = train_data['CASE_SUBMITTED_DAY'].astype('int')\n",
    "    train_data['CASE_SUBMITTED_DATE'] = train_data['CASE_SUBMITTED_DAY'].astype('str') + '-' + train_data['CASE_SUBMITTED_MONTH'].astype('str') + '-' + train_data['CASE_SUBMITTED_YEAR'].astype('str')\n",
    "    train_data['CASE_SUBMITTED_DATE'] = pd.to_datetime(train_data['CASE_SUBMITTED_DATE'], dayfirst=True, format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    train_data['DECISION_DATE'] = train_data['DECISION_DAY'].astype('str') + '-' + train_data['DECISION_MONTH'].astype('str') + '-' + train_data['DECISION_YEAR'].astype('str')\n",
    "    train_data['DECISION_DATE'] = pd.to_datetime(train_data['DECISION_DATE'], dayfirst=True, format='%d-%m-%Y', errors='coerce')\n",
    "    '''\n",
    "    Difference between CASE_SUBMITTED_DATE and DECISION_DATE date-time columns which carries information about \n",
    "    the gap (in number of days) is labelled as DECISION_PERIOD.\n",
    "    '''\n",
    "    train_data['DECISION_PERIOD'] = (train_data[\"DECISION_DATE\"] - train_data[\"CASE_SUBMITTED_DATE\"])\n",
    "    train_data['DECISION_PERIOD'] = train_data.DECISION_PERIOD.apply(lambda x: float(x.days))\n",
    "    train_data['DECISION_PERIOD'] = train_data.DECISION_PERIOD.astype('int')\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "train_data = create_decision_period_column(train_data)\n",
    "test_data = create_decision_period_column(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to impute NULL values and convert categorical values into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_fit_transform(column_name, train_data, test_data, mean_or_mode):\n",
    "    '''\n",
    "    Use MODE, if the feature is categorical.\n",
    "    Use MEAN, if the feature is numerical.\n",
    "    '''\n",
    "    if mean_or_mode == 'MODE':\n",
    "        train_data[column_name] = train_data[column_name].fillna(train_data[column_name].mode()[0])\n",
    "        test_data[column_name] = test_data[column_name].fillna(test_data[column_name].mode()[0])\n",
    "    elif mean_or_mode == 'MEAN':\n",
    "        train_data[column_name] = train_data[column_name].fillna(train_data[column_name].mean())\n",
    "        test_data[column_name] = test_data[column_name].fillna(test_data[column_name].mean())\n",
    "    '''\n",
    "    Label Encoder is used to transform categorical values into numerical values.\n",
    "    '''\n",
    "    label_encoder = le.fit(list(train_data[column_name]) + list(test_data[column_name]))\n",
    "    train_data[column_name] = label_encoder.transform(train_data[column_name]).astype('int')\n",
    "    test_data[column_name] = label_encoder.transform(test_data[column_name]).astype('int')\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns\n",
    "1. CASE_NO - This is a unique number for each CASE. Since, it does not capture any information, I dropped \n",
    "this column.\n",
    "2. EMPLOYER_NAME - This column has been removed because there are many catgories(53,463). Most of the categories \n",
    "occur only once. So, it's better to drop this feature.\n",
    "3. EMPLOYER_COUNTRY - There are 4 categories(USA, CANADA, AUSTRALIA, CHINA) in this column. Out of all Visa cases \n",
    "in training set, only 9 belong to CANADA, AUSTRALIA and CHINA. In test data, all the cases belong to USA. As this \n",
    "feature is constant in test data, this feature has been dropped.\n",
    "4. WORKSITE_POSTAL_CODE - There are 17427 categories. I have compared the performance of the models with and without \n",
    "this feature. Since, it didnot effect the score much, I dropped this feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns = ['CASE_NO', 'EMPLOYER_COUNTRY', 'EMPLOYER_NAME', 'WORKSITE_POSTAL_CODE', 'CASE_SUBMITTED_DATE', 'DECISION_DATE'])\n",
    "test_data = test_data.drop(columns = ['CASE_NO', 'EMPLOYER_COUNTRY', 'EMPLOYER_NAME', 'WORKSITE_POSTAL_CODE', 'CASE_SUBMITTED_DATE', 'DECISION_DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess each column in both train and test at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MEAN(Numerical feature), MODE(Categorical feature), NONE(Only LabelEncoding)\n",
    "'''\n",
    "train_data, test_data = fill_fit_transform('VISA_CLASS', train_data, test_data, 'NONE')\n",
    "train_data, test_data = fill_fit_transform('EMPLOYER_STATE', train_data, test_data, 'MODE')\n",
    "train_data, test_data = fill_fit_transform('WAGE_UNIT_OF_PAY', train_data, test_data, 'MODE')\n",
    "train_data, test_data = fill_fit_transform('SOC_NAME', train_data, test_data, 'NONE')\n",
    "train_data, test_data = fill_fit_transform('NAICS_CODE', train_data, test_data, 'MODE')\n",
    "train_data, test_data = fill_fit_transform('FULL_TIME_POSITION', train_data, test_data, 'MODE')\n",
    "train_data, test_data = fill_fit_transform('PREVAILING_WAGE', train_data, test_data, 'MEAN')\n",
    "train_data, test_data = fill_fit_transform('PW_UNIT_OF_PAY', train_data, test_data, 'MODE')\n",
    "train_data, test_data = fill_fit_transform('PW_SOURCE', train_data, test_data, 'MODE')\n",
    "\n",
    "train_data.loc[train_data['PW_SOURCE_YEAR'] == 1]['PW_SOURCE_YEAR'] = 2016\n",
    "train_data, test_data = fill_fit_transform('PW_SOURCE_YEAR', train_data, test_data, 'MODE')\n",
    "\n",
    "'''\n",
    "PW_SOURCE_OTHER is a categorical feature that has several categories in which most of the categories have \n",
    "occurred less than 10 times. Such categories are clubbed into one category. Around 116 categories are combined \n",
    "into one category('OTHER').\n",
    "'''\n",
    "other = list(train_data['PW_SOURCE_OTHER'].value_counts()[-116:].index)\n",
    "train_data['PW_SOURCE_OTHER'] = train_data['PW_SOURCE_OTHER'].replace(other, 'OTHER')\n",
    "test_data['PW_SOURCE_OTHER'] = test_data['PW_SOURCE_OTHER'].replace(other, 'OTHER')\n",
    "train_data, test_data = fill_fit_transform('PW_SOURCE_OTHER', train_data, test_data, 'NONE')\n",
    "\n",
    "train_data.loc[train_data['H-1B_DEPENDENT'] == 2016]['H-1B_DEPENDENT'] = 'N'\n",
    "train_data, test_data = fill_fit_transform('H-1B_DEPENDENT', train_data, test_data, 'MODE')\n",
    "\n",
    "train_data, test_data = fill_fit_transform('WILLFUL_VIOLATOR', train_data, test_data, 'MODE')\n",
    "train_data, test_data = fill_fit_transform('WORKSITE_STATE', train_data, test_data, 'NONE')\n",
    "\n",
    "train_data['WAGE_RATE_OF_PAY_FROM'] = train_data['WAGE_RATE_OF_PAY_FROM'].astype('float')\n",
    "train_data['WAGE_RATE_OF_PAY_TO'] = train_data['WAGE_RATE_OF_PAY_TO'].astype('float')\n",
    "train_data['PREVAILING_WAGE'] = train_data['PREVAILING_WAGE'].astype('float')\n",
    "\n",
    "test_data['WAGE_RATE_OF_PAY_FROM'] = test_data['WAGE_RATE_OF_PAY_FROM'].astype('float')\n",
    "test_data['WAGE_RATE_OF_PAY_TO'] = test_data['WAGE_RATE_OF_PAY_TO'].astype('float')\n",
    "test_data['PREVAILING_WAGE'] = test_data['PREVAILING_WAGE'].astype('float')\n",
    "\n",
    "train_data.ix[train_data['WAGE_RATE_OF_PAY_TO'] == 0, 'WAGE_RATE_OF_PAY_TO'] = train_data[train_data['WAGE_RATE_OF_PAY_TO'] == 0]['WAGE_RATE_OF_PAY_FROM']\n",
    "test_data.ix[test_data['WAGE_RATE_OF_PAY_TO'] == 0, 'WAGE_RATE_OF_PAY_TO'] = test_data[test_data['WAGE_RATE_OF_PAY_TO'] == 0]['WAGE_RATE_OF_PAY_FROM']\n",
    "\n",
    "'''\n",
    "A new division feature(WAGE_RATE_OF_PAY_FROM/WAGE_RATE_OF_PAY_TO) has been created from WAGE_RATE_OF_PAY_FROM and \n",
    "WAGE_RATE_OF_PAY_TO. Most of the methods capture information from the addition and subtraction of features. But, \n",
    "it is difficult to extract information from division of two features. So, I have created this feature to extract \n",
    "hidden information. This feature has increased the F1 score.\n",
    "'''\n",
    "train_data['RATIO_OF_PAY_FROM_TO'] = train_data['WAGE_RATE_OF_PAY_FROM']/train_data['WAGE_RATE_OF_PAY_TO']\n",
    "test_data['RATIO_OF_PAY_FROM_TO'] = test_data['WAGE_RATE_OF_PAY_FROM']/test_data['WAGE_RATE_OF_PAY_TO']\n",
    "\n",
    "'''\n",
    "Map categories to numerical values.\n",
    "'''\n",
    "train_data['CASE_STATUS'] = train_data['CASE_STATUS'].map({'CERTIFIED': 0, 'CERTIFIEDWITHDRAWN': 1, 'WITHDRAWN': 2, 'DENIED': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A new boolean feature IS_ES_SAMEAS_WS is added which tells whether the EMPLOYER_STATE and WORKSITE_STATE \n",
    "are the same or not.\n",
    "'''\n",
    "train_data['IS_ES_SAMEAS_WS'] = train_data['EMPLOYER_STATE'] != train_data['WORKSITE_STATE']\n",
    "train_data['IS_ES_SAMEAS_WS'] = train_data['IS_ES_SAMEAS_WS'].map({True: 1, False: 0})\n",
    "\n",
    "test_data['IS_ES_SAMEAS_WS'] = test_data['EMPLOYER_STATE'] != test_data['WORKSITE_STATE']\n",
    "test_data['IS_ES_SAMEAS_WS'] = test_data['IS_ES_SAMEAS_WS'].map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CASE_SUBMITTED_DAY</th>\n",
       "      <th>CASE_SUBMITTED_MONTH</th>\n",
       "      <th>CASE_SUBMITTED_YEAR</th>\n",
       "      <th>DECISION_DAY</th>\n",
       "      <th>DECISION_MONTH</th>\n",
       "      <th>DECISION_YEAR</th>\n",
       "      <th>VISA_CLASS</th>\n",
       "      <th>EMPLOYER_STATE</th>\n",
       "      <th>SOC_NAME</th>\n",
       "      <th>NAICS_CODE</th>\n",
       "      <th>TOTAL_WORKERS</th>\n",
       "      <th>FULL_TIME_POSITION</th>\n",
       "      <th>PREVAILING_WAGE</th>\n",
       "      <th>PW_UNIT_OF_PAY</th>\n",
       "      <th>PW_SOURCE</th>\n",
       "      <th>PW_SOURCE_YEAR</th>\n",
       "      <th>PW_SOURCE_OTHER</th>\n",
       "      <th>WAGE_RATE_OF_PAY_FROM</th>\n",
       "      <th>WAGE_RATE_OF_PAY_TO</th>\n",
       "      <th>WAGE_UNIT_OF_PAY</th>\n",
       "      <th>H-1B_DEPENDENT</th>\n",
       "      <th>WILLFUL_VIOLATOR</th>\n",
       "      <th>WORKSITE_STATE</th>\n",
       "      <th>CASE_STATUS</th>\n",
       "      <th>DECISION_PERIOD</th>\n",
       "      <th>RATIO_OF_PAY_FROM_TO</th>\n",
       "      <th>IS_ES_SAMEAS_WS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2011</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1755</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11236.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>91000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2214</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>2011</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1406</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7873.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>51000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2012</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1678</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4866.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>28455.0</td>\n",
       "      <td>28455.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>1705</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1755</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9775.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>62000.0</td>\n",
       "      <td>82000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1755</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11393.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>92000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1849</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CASE_SUBMITTED_DAY  CASE_SUBMITTED_MONTH  CASE_SUBMITTED_YEAR  \\\n",
       "0                  23                     3                 2011   \n",
       "1                  28                     3                 2011   \n",
       "2                  17                     2                 2012   \n",
       "3                  22                     3                 2012   \n",
       "4                  22                     3                 2012   \n",
       "\n",
       "   DECISION_DAY  DECISION_MONTH  DECISION_YEAR  VISA_CLASS  EMPLOYER_STATE  \\\n",
       "0            14               4           2017           1               4   \n",
       "1            10               3           2017           1              21   \n",
       "2            18              10           2016           1              18   \n",
       "3            14               4           2017           1               4   \n",
       "4            14               4           2017           1               4   \n",
       "\n",
       "   SOC_NAME  NAICS_CODE  TOTAL_WORKERS  FULL_TIME_POSITION  PREVAILING_WAGE  \\\n",
       "0         3        1755              5                   1          11236.0   \n",
       "1         0        1406              1                   1           7873.0   \n",
       "2         1        1678              1                   1           4866.0   \n",
       "3         3        1755              5                   1           9775.0   \n",
       "4         3        1755              5                   1          11393.0   \n",
       "\n",
       "   PW_UNIT_OF_PAY  PW_SOURCE  PW_SOURCE_YEAR  PW_SOURCE_OTHER  \\\n",
       "0               4          2               8               46   \n",
       "1               4          2               7               46   \n",
       "2               4          2               8               46   \n",
       "3               4          2               9               46   \n",
       "4               4          2               9               46   \n",
       "\n",
       "   WAGE_RATE_OF_PAY_FROM  WAGE_RATE_OF_PAY_TO  WAGE_UNIT_OF_PAY  \\\n",
       "0                71000.0              91000.0                 4   \n",
       "1                51000.0              51000.0                 4   \n",
       "2                28455.0              28455.0                 4   \n",
       "3                62000.0              82000.0                 4   \n",
       "4                72000.0              92000.0                 4   \n",
       "\n",
       "   H-1B_DEPENDENT  WILLFUL_VIOLATOR  WORKSITE_STATE  CASE_STATUS  \\\n",
       "0               1                 0               4            1   \n",
       "1               0                 0              21            1   \n",
       "2               0                 0              18            2   \n",
       "3               1                 0               4            1   \n",
       "4               1                 0               4            1   \n",
       "\n",
       "   DECISION_PERIOD  RATIO_OF_PAY_FROM_TO  IS_ES_SAMEAS_WS  \n",
       "0             2214              0.780220                0  \n",
       "1             2174              1.000000                0  \n",
       "2             1705              1.000000                0  \n",
       "3             1849              0.756098                0  \n",
       "4             1849              0.782609                0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample lower class data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Since the dataset provided is imbalanced, I manually up-sampled the data-points with lower class by 20 times.\n",
    "'''\n",
    "class_two_data = train_data[train_data['CASE_STATUS'] == 2]\n",
    "\n",
    "class_three_data = train_data[train_data['CASE_STATUS'] == 3]\n",
    "copy_of_class_three_data = class_three_data.copy()\n",
    "for i in range(1, 20):\n",
    "    class_three_data = class_three_data.append(copy_of_class_three_data, ignore_index=False)\n",
    "\n",
    "train_data = train_data.append(class_three_data, ignore_index=False)\n",
    "train_data = train_data.append(class_two_data, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X_train and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['CASE_STATUS'])\n",
    "y_train = train_data['CASE_STATUS']\n",
    "'''\n",
    "Split the dataset into train and validation sets.\n",
    "'''\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n",
    "dummy_test_data = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=12,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_1 = xgb.XGBClassifier(objective = 'multi:softmax', \n",
    "                              n_estimators = 100, \n",
    "                              max_depth = 3,\n",
    "                              colsample_bylevel = 0.9,\n",
    "                              learning_rate = 0.1,\n",
    "                              random_state=12)\n",
    "\n",
    "xgb_model_1.fit(X_train, y_train, eval_set=[(val_X, val_y)], verbose=False, early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score - 0.9773992102961836\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred_1 = xgb_model_1.predict(val_X)\n",
    "print ('F1 score - ' + str(f1_score(y_valid_pred_1, val_y, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = xgb_model_1.predict(test_data)\n",
    "y_pred_1 = pd.DataFrame({'CASE_NO': dummy_test_data['CASE_NO'], 'CASE_STATUS': y_pred_1})\n",
    "y_pred_1['CASE_STATUS'] = y_pred_1['CASE_STATUS'].map({0: 'CERTIFIED', 1: 'CERTIFIEDWITHDRAWN', 2: 'WITHDRAWN', 3: 'DENIED'})\n",
    "y_pred_1.to_csv('1_prediction_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=1, missing=None, n_estimators=80,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=12,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_2 = xgb.XGBClassifier(objective = 'multi:softmax', \n",
    "                              n_estimators = 80, \n",
    "                              max_depth = 5,\n",
    "                              colsample_bylevel = 0.9,\n",
    "                              learning_rate = 0.1,\n",
    "                              random_state=12)\n",
    "\n",
    "xgb_model_2.fit(X_train, y_train, eval_set=[(val_X, val_y)], verbose=False, early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score - 0.9797662946952419\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred_2 = xgb_model_2.predict(val_X)\n",
    "print ('F1 score - ' + str(f1_score(y_valid_pred_2, val_y, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = xgb_model_2.predict(test_data)\n",
    "y_pred_2 = pd.DataFrame({'CASE_NO': dummy_test_data['CASE_NO'], 'CASE_STATUS': y_pred_2})\n",
    "y_pred_2['CASE_STATUS'] = y_pred_2['CASE_STATUS'].map({0: 'CERTIFIED', 1: 'CERTIFIEDWITHDRAWN', 2: 'WITHDRAWN', 3: 'DENIED'})\n",
    "y_pred_2.to_csv('2_prediction_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=2, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=12,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model_3 = xgb.XGBClassifier(objective = 'multi:softmax', \n",
    "                              n_estimators = 100, \n",
    "                              max_depth = 2,\n",
    "                              colsample_bylevel = 1,\n",
    "                              learning_rate = 0.01,\n",
    "                              random_state=12)\n",
    "\n",
    "xgb_model_3.fit(X_train, y_train, eval_set=[(val_X, val_y)], verbose=False, early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score - 0.9702938143165597\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred_3 = xgb_model_3.predict(val_X)\n",
    "print ('F1 score - ' + str(f1_score(y_valid_pred_3, val_y, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_3 = xgb_model_3.predict(test_data)\n",
    "y_pred_3 = pd.DataFrame({'CASE_NO': dummy_test_data['CASE_NO'], 'CASE_STATUS': y_pred_3})\n",
    "y_pred_3['CASE_STATUS'] = y_pred_3['CASE_STATUS'].map({0: 'CERTIFIED', 1: 'CERTIFIEDWITHDRAWN', 2: 'WITHDRAWN', 3: 'DENIED'})\n",
    "y_pred_3.to_csv('3_prediction_xgboost.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble = pd.DataFrame({'1': y_pred_1.CASE_STATUS, '2': y_pred_2.CASE_STATUS,\n",
    "                         '3': y_pred_3.CASE_STATUS})\n",
    "case_status = np.array(Ensemble.mode(axis=1))[:, 0]\n",
    "y_pred = pd.DataFrame({'CASE_NO': y_pred_1.CASE_NO,'CASE_STATUS': case_status})\n",
    "y_pred.to_csv('ensemble_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
